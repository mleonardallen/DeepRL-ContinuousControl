{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import accumulate\n",
    "from operator import itemgetter\n",
    "\n",
    "import env_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher.app\", no_graphics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.12099999729543924\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choose a learning Algorithm\n",
    "\n",
    "For [Project 1: Navigation](https://github.com/mleonardallen/nd893-p1-navigation), I leveraged the [DQN Algorithm](https://arxiv.org/abs/1312.5602).\n",
    "\n",
    "> We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n",
    "\n",
    "DQN is a value based approach, seeking the optimal `action-value` function.  Performing an action with value based approaches involves an `argmax` operation to find the action with the highest value.  However, the `Reacher` environment has a `continuous action space`, and value based approaches are not suited to continuous action spaces because of this argmax operation.\n",
    "\n",
    "We could potentially discretize the action space, but I don't think this will be effective.  Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.  Even with a small amount of granularity in torque values produces a large number of possible actions.\n",
    "\n",
    "| Torque Values | Actions |\n",
    "| -- | -- |\n",
    "| -1.0, -0.5, 0.0, 0.5, 1.0 | 625 |\n",
    "| -1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0 | 6,561 |\n",
    "\n",
    "Because of the limitations of values based methods for continues action spaces, I think we need to use a policy based approach for the Reacher environment.  Policy based approaches seek to estimate the optimal policy directly, and therefore we don't run into this argmax issue that we have with value based approaches.\n",
    "\n",
    "To begin, I'm going to keep it very simple using the [REINFORCE Algorithm](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.8871).  REINFORCE shows us how to adjust parameters so that good actions are more probable and bad actions are less probable.  Likely, this algorithm will do poorly with this environment, but we can see the results and add improvements as we go. The REINFORCE algorithm is summarized below.\n",
    "\n",
    "1. Use the policy $\\pi_\\theta$ to collect $m$ trajectories {$\\tau^{(1)},\\tau^{(2)},\\dots,\\tau^{(m)}$} with horizon $H$.  We refer to the $i$-th trajectory as \n",
    "\n",
    "    $\\tau^{(i)} = (s^{(i)}_0, a^{(i)}_0,\\dots,s^{(i)}_H, a^{(i)}_H,s^{(i)}_{H+1})$\n",
    "\n",
    "\n",
    "2. Use the trajectories to estimate the gradient $\\nabla_\\theta U(\\theta)$:\n",
    "\n",
    "    $\\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac1m\\sum_{i=1}^m\\sum_{t=0}^H\\nabla_\\theta log\\pi_\\theta(a_t^{(i)}|s_t^{(i)}R(\\tau^{(i)})$\n",
    "\n",
    "\n",
    "3. Update the weights of the policy:\n",
    "\n",
    "    $\\theta \\leftarrow \\theta + \\alpha\\hat{g}$\n",
    "\n",
    "\n",
    "4. Loop over steps 1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replace the Random Actions with Random Actions ðŸ™ƒ\n",
    "\n",
    "<img align=\"right\" alt=\"tanh\" title=\"tanh\" src=\"img/tanh.png\" width=\"250\"/>\n",
    "\n",
    "Since this is my first project involving continuous actions spaces, I want to build the project step by step, until I have an agent that can efficiently learn and solve the environment.\n",
    "\n",
    "First, I just want to replace the random actions produced by `np.random.randn` with a neural network that outputs random actions.  The actions are random because the network is untrained.  Theoretically though, the trained network could produce optimal actions.\n",
    "\n",
    "Note: The output layer uses [tanh](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh) to produce the desired torque application values (-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]           2,176\n",
      "            Linear-2                [-1, 1, 64]           4,160\n",
      "            Linear-3                 [-1, 1, 4]             260\n",
      "================================================================\n",
      "Total params: 6,596\n",
      "Trainable params: 6,596\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        self.layers.extend([nn.Linear(i, o) for i, o in zip(hidden_layers[:-1], hidden_layers[1:])])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.elu(layer(x))\n",
    "        return torch.tanh(self.output(x))\n",
    "\n",
    "policy = Network(state_size, action_size, [64, 64])\n",
    "summary(policy, (1, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.4599999897181988\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "while True:\n",
    "    actions = policy(torch.from_numpy(states).float()).detach().numpy() # actions come from network\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/random.gif\" />\n",
    "\n",
    "The 20 agent environment is a bit overwhelming visually, but we can see that the agent behavior is quite random, and we can tell agent is not doing that well.  Bright green indicates the  agent's hand is in the goal location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Update the Network to work with REINFORCE\n",
    "\n",
    "<img align=\"right\" alt=\"tanh\" title=\"tanh\" src=\"img/softplus.png\" width=\"250\"/>\n",
    "\n",
    "REINFORCE shows us how to adjust parameters so that good actions are more probable and bad actions are less probable.  To achieve this, the network needs to output probabilities, and currently the network only outputs actions.\n",
    "\n",
    "I make a small tweak to the network so that it returns the action AND probabiliy of taking that action.  The original output becomes the `mean` and I use that mean to create a `normal distribution`.\n",
    "\n",
    "Note: When creating the normal distribution I use a [softplus](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) operation.  This is because standard deviations cannot be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]           2,176\n",
      "            Linear-2                [-1, 1, 64]           4,160\n",
      "            Linear-3                 [-1, 1, 4]             260\n",
      "================================================================\n",
      "Total params: 6,596\n",
      "Trainable params: 6,596\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        self.layers.extend([nn.Linear(i, o) for i, o in zip(hidden_layers[:-1], hidden_layers[1:])])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.std = nn.Parameter(torch.zeros(output_size)) # parameter for standard deviation\n",
    "    \n",
    "    def forward(self, x, action=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.elu(layer(x))\n",
    "        mean = torch.tanh(self.output(x)) # original output becomes mean of distribution\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std)) # create distribution with mean, std\n",
    "        if action is None:\n",
    "            action = dist.sample() # action sampled from distribution\n",
    "        log_prob = dist.log_prob(action) # probabilities required for REINFORCE\n",
    "        entropy = dist.entropy()\n",
    "        return {'mean': mean, 'action': action, 'log_prob': log_prob, 'entropy': entropy}\n",
    "\n",
    "policy = Network(state_size, action_size, [64, 64])\n",
    "summary(policy, (1, state_size)) # summary is not including std Â¯\\_(ãƒ„)_/Â¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Build the REINFORCE Agent\n",
    "\n",
    "I impliment the REINFORCE algorithm below.  For the Reacher environment, I do not think REINFORCE can learn a decent policy, but let's check it out and see how it performs.\n",
    "\n",
    "Note: At this point, I extracted out main loop into a `env_utils` file because I just want to focus on the details of the algorithm here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.policy = Network(state_size, action_size, [64, 64])\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4)\n",
    "        self.storage = []\n",
    "    \n",
    "    def act(self, states):\n",
    "        return self.policy(torch.from_numpy(states).float())\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        self.storage.append((actions['log_prob'], rewards))\n",
    "        if np.any(dones):\n",
    "            self.learn()\n",
    "            \n",
    "    def learn(self):\n",
    "        log_probs, rewards = zip(*self.storage)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "\n",
    "        \"\"\" Sum Rewards\n",
    "        Pretend Example: 4 agents, each with a trajectory over 3 time steps\n",
    "        \n",
    "            axis=0\n",
    "              â†“ \n",
    "            [[1, 0, 0, 0], t=0\n",
    "             [1, 0, 1, 0], t=1\n",
    "             [1, 0, 1, 1]] t=2\n",
    "             ------------\n",
    "        sum: [3, 0, 2, 1]\n",
    "        \n",
    "        \"\"\"\n",
    "        R = np.sum(rewards, axis=0)[:,np.newaxis]\n",
    "        R = torch.from_numpy(R).float()\n",
    "\n",
    "        policy_loss = torch.mean(-log_probs * R) # negative because optimizer uses gradient descent\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.storage = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25, Total score (averaged over agents): 1.1199999749660492\n",
      "Episode: 50, Total score (averaged over agents): 1.3569999696686863\n",
      "Episode: 75, Total score (averaged over agents): 1.1334999746643006\n",
      "Episode: 100, Total score (averaged over agents): 1.2654999717138709\n",
      "Episode: 125, Total score (averaged over agents): 1.3549999697133899\n",
      "Episode: 150, Total score (averaged over agents): 1.398499968741089\n",
      "Episode: 175, Total score (averaged over agents): 1.1274999747984111\n",
      "Episode: 200, Total score (averaged over agents): 1.23149997247383\n",
      "Episode: 225, Total score (averaged over agents): 1.0889999756589532\n",
      "Episode: 250, Total score (averaged over agents): 1.3504999698139728\n",
      "Episode: 275, Total score (averaged over agents): 1.4299999680370092\n",
      "Episode: 300, Total score (averaged over agents): 1.069499976094812\n",
      "Episode: 325, Total score (averaged over agents): 1.192499973345548\n",
      "Episode: 350, Total score (averaged over agents): 0.99899997767061\n",
      "Episode: 375, Total score (averaged over agents): 1.0864999757148326\n",
      "Episode: 400, Total score (averaged over agents): 1.4194999682717024\n",
      "Episode: 425, Total score (averaged over agents): 1.6004999642260374\n",
      "Episode: 450, Total score (averaged over agents): 1.6014999642036856\n",
      "Episode: 475, Total score (averaged over agents): 1.2569999719038605\n",
      "Episode: 500, Total score (averaged over agents): 1.5709999648854136\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {}\n",
    "config['env'] = env\n",
    "config['print_every'] = 25\n",
    "config['plot_every'] = 10\n",
    "config['episodes'] = 500\n",
    "agent = Agent(config)\n",
    "env_utils.run(agent)\n",
    "torch.save(agent.policy.state_dict(), 'checkpoints/REINFORCE-1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['env'] = env\n",
    "config['train_mode'] = False\n",
    "agent = Agent(config)\n",
    "checkpoint = torch.load('checkpoints/REINFORCE-1.pth')\n",
    "agent.policy.load_state_dict(checkpoint)\n",
    "env_utils.run(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/REINFORCE-1.gif\" />\n",
    "\n",
    "Updates appear to be very noisy, and the average reward is very low even after training for 500 episides. When I watch the agent, it's clear that agent is barely doing better than taking random actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. REINFORCE Improvements\n",
    "\n",
    "What are the main problems of REINFORCE? There are three issues:\n",
    "\n",
    "1. The update process is very inefficient! We run the policy once, update once, and then throw away the trajectory.\n",
    "\n",
    "2. The gradient estimate $\\hat{g}$ is very noisy. By chance the collected trajectory may not be representative of the policy.\n",
    "\n",
    "3. There is no clear credit assignment. A trajectory may contain many good/bad actions and whether these actions are reinforced depends only on the final total output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.gamma = 0.99\n",
    "        self.beta = 0.01 # entropy regularization\n",
    "        self.beta_decay = 0.995\n",
    "        self.policy = Network(state_size, action_size, [64, 64])\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4)\n",
    "        self.storage = []\n",
    "        \n",
    "    def act(self, states):\n",
    "        return self.policy(torch.from_numpy(states).float())\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        self.storage.append((actions['log_prob'], actions['entropy'], rewards))\n",
    "        if np.any(dones):\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        log_probs, entropy, rewards = zip(*self.storage)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        entropy = torch.stack(entropy)\n",
    "\n",
    "        \"\"\" Clear Credit Assignment: Discounted Future Rewards\n",
    "        Note: arr[::-1] is just syntax to reverse an array.\n",
    "        Calculating the discounted future reward is easier starting at the end.\n",
    "        \"\"\"\n",
    "        accumulate_fn = lambda acc,r: r+self.gamma*np.asarray(acc)\n",
    "        R = np.array(list(accumulate(rewards[::-1], accumulate_fn)))[::-1]\n",
    "        \n",
    "        \"\"\" Less Noisy: Normalize Rewards\n",
    "        Batch normalization is a technique for improving the speed, performance, and stability of a network.\n",
    "        https://en.wikipedia.org/wiki/Batch_normalization\n",
    "\n",
    "        Before normalizing, rewards were all positive for the Reacher environment.\n",
    "        The normalized distribution, with Î¼ = 0, produces both positive and negative rewards.\n",
    "        Perhaps this lack of negative reward made it difficult to decrease probabilities of bad actions.\n",
    "        \"\"\"\n",
    "        R = (R - np.mean(R))/np.std(R)\n",
    "        R = torch.from_numpy(R.copy()).float().unsqueeze(2)\n",
    "\n",
    "        \"\"\" Entropy Regularization to Encourage Exploration\n",
    "        On-policy Reinforcement Learning with Entropy Regularization\n",
    "        https://arxiv.org/abs/1912.01557\n",
    "        \"\"\"\n",
    "        policy_loss = torch.mean(-log_probs * R - self.beta * entropy)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.beta *= self.beta_decay\n",
    "        self.storage = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25, Total score (averaged over agents): 1.6944999621249734\n",
      "Episode: 50, Total score (averaged over agents): 2.187499951105565\n",
      "Episode: 75, Total score (averaged over agents): 2.3834999467246236\n",
      "Episode: 100, Total score (averaged over agents): 3.497499921824783\n",
      "Episode: 125, Total score (averaged over agents): 3.861999913677573\n",
      "Episode: 150, Total score (averaged over agents): 3.945499911811203\n",
      "Episode: 175, Total score (averaged over agents): 4.359999902546406\n",
      "Episode: 200, Total score (averaged over agents): 5.460499877948314\n",
      "Episode: 225, Total score (averaged over agents): 5.594999874942005\n",
      "Episode: 250, Total score (averaged over agents): 7.093999841436744\n",
      "Episode: 275, Total score (averaged over agents): 6.34999985806644\n",
      "Episode: 300, Total score (averaged over agents): 7.918499823007733\n",
      "Episode: 325, Total score (averaged over agents): 7.9799998216331005\n",
      "Episode: 350, Total score (averaged over agents): 8.39149981243536\n",
      "Episode: 375, Total score (averaged over agents): 9.046499797794969\n",
      "Episode: 400, Total score (averaged over agents): 9.637499784585088\n",
      "Episode: 425, Total score (averaged over agents): 9.396999789960683\n",
      "Episode: 450, Total score (averaged over agents): 10.479499765764922\n",
      "Episode: 475, Total score (averaged over agents): 10.866499757114799\n",
      "Episode: 500, Total score (averaged over agents): 10.501499765273184\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {}\n",
    "config['env'] = env\n",
    "config['print_every'] = 25\n",
    "config['plot_every'] = 10\n",
    "config['episodes'] = 500\n",
    "agent = Agent(config)\n",
    "env_utils.run(agent)\n",
    "torch.save(agent.policy.state_dict(), 'checkpoints/REINFORCE-2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['env'] = env\n",
    "config['train_mode'] = False\n",
    "agent = Agent(config)\n",
    "checkpoint = torch.load('checkpoints/REINFORCE-2.pth')\n",
    "agent.policy.load_state_dict(checkpoint)\n",
    "env_utils.run(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/REINFORCE-2.gif?2\" />\n",
    "\n",
    "The agent learned some strategies!  It's doing a bit better than the first iteration of REINFORCE, but it's taking a long time to train, and I can still see long ways to go before they agent performs optimially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Proximal Policy Optimization (PPO)\n",
    "\n",
    "The updates to REINFORCE improved learning, but the agent is still taking a long time to learn the environment.  Recall one issue with REINFORCE is the update process is very inefficient! We run the policy once, update once, and then throw away the trajectory.\n",
    "\n",
    "To tackle this issue, I impliment the [PPO algorithm](https://arxiv.org/abs/1707.06347).\n",
    "\n",
    "> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.\n",
    "\n",
    "\n",
    "The PPO algorithm is summarized below.\n",
    "\n",
    "1. First, collect some trajectories based on some policy $\\pi_{\\theta}$, and initialize theta prime $\\theta'=\\theta$\n",
    "2. Next, compute the gradient of the clipped surrogate function using the trajectories\n",
    "3. Update $\\theta'$ using gradient ascent $\\theta'\\leftarrow\\theta' +\\alpha \\nabla_{\\theta'}L_{\\rm sur}^{\\rm clip}(\\theta', \\theta)$\n",
    "4. Then we repeat step 2-3 without generating new trajectories. Typically, step 2-3 are only repeated a few times\n",
    "5. Set $\\theta=\\theta'$, go back to step 1, repeat.\n",
    "\n",
    "The clipped surrogate function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.gamma = 0.99\n",
    "        self.beta = 0.01\n",
    "        self.beta_decay = 0.995\n",
    "        self.epsilon = 0.1 # parameter used for clipping\n",
    "        self.sgd_epoch = 8 # number of times to use the trajectories\n",
    "        self.policy = Network(state_size, action_size, [64, 64])\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4)\n",
    "        self.storage = []\n",
    "        \n",
    "    def act(self, states):\n",
    "        return self.policy(torch.from_numpy(states).float())\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        self.storage.append((states, actions['action'], actions['log_prob'], rewards))\n",
    "        if np.any(dones):\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        states, actions, log_probs, rewards = zip(*self.storage)\n",
    "        states = torch.from_numpy(np.asarray(states)).float()\n",
    "        actions = torch.stack(actions)\n",
    "        log_probs = torch.stack(log_probs).detach() # NOTE: detach because we only update current gradient\n",
    "        \n",
    "        accumulate_fn = lambda acc,r: r+self.gamma*np.asarray(acc)\n",
    "        R = np.array(list(accumulate(rewards[::-1], accumulate_fn)))[::-1]\n",
    "        R = (R - np.mean(R))/np.std(R)\n",
    "        R = torch.from_numpy(R.copy()).float().unsqueeze(2)  \n",
    "\n",
    "        for e in range(self.sgd_epoch):\n",
    "\n",
    "            prediction = self.policy(states, actions)\n",
    "\n",
    "            \"\"\" Calculate the Ratio\n",
    "            Although the PPO algoritm indicates the use of a ratio, here I use log probabilities. \n",
    "            The quotient rule for logarithms:  ð‘™ð‘œð‘”_b(ð‘›/ð‘š) = ð‘™ð‘œð‘”_ð‘(ð‘›) âˆ’ ð‘™ð‘œð‘”_ð‘(ð‘š)\n",
    "            Therefore: ð‘’^(log_e(ðœ‹ðœƒâ€²) âˆ’ log_e(ðœ‹ðœƒ)) = ðœ‹ðœƒâ€²/ðœ‹ðœƒ\n",
    "            \"\"\" \n",
    "            ratio = (prediction['log_prob'] - log_probs).exp()\n",
    "\n",
    "            \"\"\" The clip function is implemented in pytorch as\n",
    "            torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.clamp\n",
    "            \"\"\"\n",
    "            obj = ratio\n",
    "            obj_clipped = ratio.clamp(1.0 - self.epsilon, 1.0 + self.epsilon)\n",
    "\n",
    "            L = -torch.min(obj * R, obj_clipped * R).mean() - self.beta * prediction['entropy'].mean()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            self.optimizer.step()\n",
    "            del L\n",
    "\n",
    "        self.beta *= self.beta_decay\n",
    "        self.storage = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25, Total score (averaged over agents): 2.4959999442100527\n",
      "Episode: 50, Total score (averaged over agents): 4.243499905150384\n",
      "Episode: 75, Total score (averaged over agents): 6.068999864347279\n",
      "Episode: 100, Total score (averaged over agents): 8.074499819520861\n",
      "Episode: 125, Total score (averaged over agents): 9.487999787926674\n",
      "Episode: 150, Total score (averaged over agents): 9.599499785434455\n",
      "Episode: 175, Total score (averaged over agents): 8.74399980455637\n",
      "Episode: 200, Total score (averaged over agents): 10.43199976682663\n",
      "Episode: 225, Total score (averaged over agents): 12.432999722100794\n",
      "Episode: 250, Total score (averaged over agents): 12.700999716110527\n",
      "Episode: 275, Total score (averaged over agents): 12.602499718312174\n",
      "Episode: 300, Total score (averaged over agents): 14.002499687019736\n",
      "Episode: 325, Total score (averaged over agents): 14.637999672815203\n",
      "Episode: 350, Total score (averaged over agents): 14.052999685890972\n",
      "Episode: 375, Total score (averaged over agents): 15.249499659147114\n",
      "Episode: 400, Total score (averaged over agents): 14.397499678190798\n",
      "Episode: 425, Total score (averaged over agents): 15.193999660387636\n",
      "Episode: 450, Total score (averaged over agents): 15.65699965003878\n",
      "Episode: 475, Total score (averaged over agents): 16.72999962605536\n",
      "Episode: 500, Total score (averaged over agents): 17.086999618075787\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {}\n",
    "config['env'] = env\n",
    "config['print_every'] = 25\n",
    "config['plot_every'] = 10\n",
    "config['episodes'] = 500\n",
    "agent = Agent(config)\n",
    "env_utils.run(agent)\n",
    "torch.save(agent.policy.state_dict(), 'checkpoints/PPO-1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['env'] = env\n",
    "config['train_mode'] = False\n",
    "agent = Agent(config)\n",
    "checkpoint = torch.load('checkpoints/PPO-1.pth')\n",
    "agent.policy.load_state_dict(checkpoint)\n",
    "env_utils.run(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/PPO-1.gif\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
